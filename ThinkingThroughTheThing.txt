This is going to be a VERY unorganized txt file. Also, please notice the alliteration in the name 
of this file.

I intend to keep a log of what I am thinking along the way in creating this system. I will often 
not think in the correct order and will jump around and ahead and refactor things idk. Also will include 
slang/abbreviations/short hands/probably incorrect terminology and spellings etc. This is purely intended for future
me to see how I was thinking in this time period.

---------

Okay first I am already thinking about how to tackle PDF embeddings.
https://www.anthropic.com/news/contextual-retrieval - this seems good but too slow and heavy. probably 
wont use this.
https://jina.ai/news/late-chunking-in-long-context-embedding-models/ - seems promising. may use this somehow.
will see how.

I am thinking using Qwen3 0.6B - very high on MTEB, fairly small (~600M) and multilingual with a 32K
context window. Nice.

Unstructured seems to be the library I think I will use for parsing/chunking. especially the hi_res mode 
looks very powerful.

hold on if i need to include images, I need a multimodal model. will have to check mmeb leaderboard.


file system/adding to vector db
- will have to calculate a file hash to make sure the file is not updates since the prev embedding
- if a file does not exist anymore, the vector db entries to be removed
- multiple processes for subdirectories? could work?
- I think one db with one big collection with metadata filtering can work

plan for now - 

one db with one collection at a fixed file location. collecn will look like - 
id, embedding, {filepath, last modified time, filename}

given a path,

get sets of files and subdirs from disk with last modified
get sets of files and subdirs from db
find embeddings to delete
find files/subdirs to embed and add
find files/subdirs to update (delete old embeddings and then crate new ones)

if the path not in db at all (no embeddings generated), embed and go to search

if path in db go to search. new files to be added have been added already.

for search add condition/conditions such that search is limited to curr and sub dirs

will use "jinaai/jina-clip-v2" as the embedding model since this supports embedding
text and images. must be careful around diff file types when encoding since giving
only the file path name may encode the string itself.

chroma requires ids when adding embeddings since one doc can have multiple embeddings
also chroma docs say - 
    you can supply a list of document-associated embeddings directly, and Chroma will store the 
    associated documents without embedding them itself. Note that in this case 
    there will be no guarantee that the embedding is mapped to the document associated with it.
what? why would they do that? i want to create embeddings in batches separately
must implement a custom embedding function

for images the documents in chroma has cannot be a blob so in case of images will have to keep the name of the file only. and as far as I can tell the library allows for
images to be embedded with the filename only. so no need to handle images separately.

when scanning files, ignore foles and folders that start with "."

originally wanted to delete entire subdirs (if deleted from fs) with one command but seems like regex is not supported in metadata filtering sp maybe not

wow chroma does not support any kind of prefix for "like" when metadata filtering so preppring the db for search will mean doing everything manually

just had the genius idea of wrapping everything in a class why didnt i think of this earlier

cant think of how to use late chunking here so will stick to semi-context aware emebeddings (model context limitations)

once the fs state is prepared will use that to get the mod times since we already have that.

to facilitate the search will also store path and filename in metadata cause that can be used with the eq condition seems like the best way